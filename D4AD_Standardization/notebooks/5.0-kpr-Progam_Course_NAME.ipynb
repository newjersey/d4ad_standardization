{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# Importing the libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "# active labeler related\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import ComplementNB  # corrects for class imbalance, SGD is pretty good too\n",
    "from sklearn.pipeline import Pipeline\n",
    "from superintendent import ClassLabeller\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='char', ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', ComplementNB()),\n",
    "])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# Set up columns to keep, fields, locations for writing\n",
    "rootpath = \"/hdd/work/d4ad_standardization/\"\n",
    "processedpath = \"D4AD_Standardization/data/processed/\"\n",
    "externalpath = \"D4AD_Standardization/data/external/\"\n",
    "interimpath = \"D4AD_Standardization/data/interim/\"\n",
    "\n",
    "content_is = \"standardized_name_and_name1\"\n",
    "\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "\n",
    "filepath = \"./D4AD_Standardization/data/raw/etpl_all_programsJune3.xls\"\n",
    "filepath = \"standardized_name.csv\" # builds off of notebook 3 work\n",
    "\n",
    "columns = [\n",
    "    \"NAME_1\",\n",
    "    \"STANDARDIZEDNAME\",\n",
    "    \"NAME\",\n",
    "    \"DESCRIPTION\",\n",
    "    \"PREREQUISITES\",\n",
    "    \"FEATURESDESCRIPTION\",\n",
    "    \"STREET1\",\n",
    "    \"CITY\",\n",
    "    \"STATE\",\n",
    "    \"ZIP\",\n",
    "    \"WEBSITE\",\n",
    "    \"COUNTY\",\n",
    "    \"NONGOVAPPROVAL\",\n",
    "    \"STATECOMMENTS\",\n",
    "    \"CIPCODE\",\n",
    "    \"PROVIDERID\",\n",
    "    \"APPROVINGAGENCYID\"\n",
    "]\n",
    "\n",
    "columns_to_save = ['STANDARDIZEDNAME_1'] + columns\n",
    "\n",
    "SKIP_THIS = True # helps me be able to run all and not worry about pulling things\n",
    "# I already know I have on disk\n",
    "\n",
    "#df = pd.read_excel(rootpath + interimpath + filepath, usecols=columns)\n",
    "df = pd.read_csv(rootpath + interimpath + filepath, usecols=columns)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "if not SKIP_THIS:\n",
    "    ONET_TOOLS_TECH_URL_NAME = (\"https://www.onetcenter.org/dl_files/database/db_20_1_text/Tools%20and%20Technology.txt\", \"onet_tools_tech.csv\")\n",
    "    CAREERONESTOP_CERTIFICATIONS_URL_NAME = (\"https://www.careeronestop.org/TridionMultimedia/tcm24-48614_CareerOnestop_Certifications_07072020.zip\", \"career_one_stop.zip\")\n",
    "\n",
    "    filepath = rootpath + externalpath\n",
    "\n",
    "    for dataset in (ONET_TOOLS_TECH_URL_NAME, CAREERONESTOP_CERTIFICATIONS_URL_NAME):\n",
    "        url, filename = dataset\n",
    "        print(\"running ...\", f'\\nwget -O {filepath+filename} {url}')\n",
    "        os.system(f'wget -O {filepath+filename} {url}')\n",
    "        print(\"filetype is\",  filename[-3:])\n",
    "\n",
    "        if filename[-3:] == 'zip':\n",
    "            with zipfile.ZipFile(filepath+filename,\"r\") as zip_ref:\n",
    "                zipdir = filepath+filename[:-4]\n",
    "                print(\"unzipping {} to ...\".format(filename), zipdir)\n",
    "                os.mkdir(zipdir)\n",
    "                zip_ref.extractall(zipdir)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', False)\n",
    "\n",
    "the_df = df #df.sample(n=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# A) \n",
    "# The program or course name can start or end with a matching parenthesis. In these cases\n",
    "# we assume that no other matching parenthesis are present and apply \n",
    "# an appropriate regex for that...\n",
    "\n",
    "# First, set up standardized column with default values\n",
    "the_df[\"STANDARDIZEDNAME_1\"] = \"\"\n",
    "\n",
    "# ... then extract names for those with opening parens\n",
    "open_parenthesis_index = the_df.NAME_1.str[0] == '('\n",
    "open_parenthesis_regex = '''\n",
    "                (?P<paren>\\(.*\\)) # get the first parathesis\n",
    "                (?P<the_name>.*)  # then get the actual name\n",
    "                '''\n",
    "\n",
    "the_df.loc[open_parenthesis_index, \"STANDARDIZEDNAME_1\"] =\\\n",
    "    the_df.loc[open_parenthesis_index, 'NAME_1']\\\n",
    "          .str\\\n",
    "          .extract(open_parenthesis_regex, flags=re.VERBOSE).the_name\n",
    "\n",
    "# ... then extract names for those with closing parens\n",
    "close_parenthesis_index = the_df.NAME_1.str[-1] == ')'\n",
    "closing_parenthesis_regex = '''\n",
    "                (?P<the_name>.*)  # get the actual name\n",
    "                (?P<paren>\\(.*\\)) # get the last parathensis                \n",
    "                '''\n",
    "the_df.loc[close_parenthesis_index, \"STANDARDIZEDNAME_1\"] =\\\n",
    "    the_df.loc[close_parenthesis_index, 'NAME_1']\\\n",
    "          .str\\\n",
    "          .extract(closing_parenthesis_regex, flags=re.VERBOSE).the_name\n",
    "\n",
    "# ... then we copy over content that has a internal parenthesis with those\n",
    "# parenthesis removed and ignore everything after, e.g. \"ABC (123) DEF\" --> \"ABC\"\n",
    "internal_parenthesis_index =\\\n",
    "    the_df['NAME_1'].str.contains('\\(|\\)', regex=True) &\\\n",
    "        ~(close_parenthesis_index|open_parenthesis_index)\n",
    "\n",
    "the_df.loc[internal_parenthesis_index, \"STANDARDIZEDNAME_1\"] =\\\n",
    "    the_df.loc[internal_parenthesis_index, 'NAME_1']\\\n",
    "          .str\\\n",
    "          .extract(closing_parenthesis_regex, flags=re.VERBOSE).the_name\n",
    "\n",
    "# ... finally, just copy over everything else\n",
    "no_parenthesis_index = ~(close_parenthesis_index |\\\n",
    "                         open_parenthesis_index  |\\\n",
    "                         internal_parenthesis_index)\n",
    "the_df.loc[no_parenthesis_index, \"STANDARDIZEDNAME_1\"] =\\\n",
    "    the_df.loc[no_parenthesis_index, 'NAME_1']\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "constructed abbreviation list...\ndone\n"
    }
   ],
   "source": [
    "# 2)\n",
    "# So now we have silver version data of program, course names\n",
    "# from the cell above, in STANDARDIZEDNAME_1\n",
    "#\n",
    "# To make an incrementally better version we need to expand \n",
    "# abbreviations and acroynmns.\n",
    "\n",
    "# **We do this if we don't skip things and the labeling file does not exist**\n",
    "\n",
    "# Here I identify two pretty common cases of acronymns and abbreviations:\n",
    "#   All caps\n",
    "#   Xx*. <- capitalized inital letter ending with a period\n",
    "\n",
    "# Now let's attempt to extract presumed acronyms and see if we can\n",
    "# directly label them. I assume there are far fewer unique abbreviations\n",
    "# so that a person can actually do this in a short amount of time\n",
    "abbreviation_pickle = rootpath + interimpath + 'abbreviation_label.pickle'\n",
    "\n",
    "if os.path.exists(abbreviation_pickle):\n",
    "    flags = re.VERBOSE\n",
    "\n",
    "    #  TODO: check if abbreviation labeled file already exists, if it does\n",
    "    # we skip this portion\n",
    "\n",
    "    # Pandas/Python doesn't like this verbose regex but likes other?\n",
    "    # all_caps_regex = '''\n",
    "    #                 \\b(?P<all_caps>[A-Z]+)  # Get all caps words\n",
    "    #                 [\\s,:\\d]                # sit before a space, comma or digit\n",
    "    #                 '''\n",
    "    all_caps_regex = r'\\b(?P<all_caps>[A-Z]+)[\\s,:\\d]'\n",
    "\n",
    "    dotted_word_regex = r'(?P<dot_abbreviation>[A-Z][a-z]+\\.)'\n",
    "    dotted_word_regex =\\\n",
    "        \"\"\"\n",
    "        (?P<dot_abbreviation>[a-zA-Z][a-z]+\\.)\n",
    "        \"\"\"\n",
    "\n",
    "    the_regexs = \"|\".join([all_caps_regex, dotted_word_regex])\n",
    "\n",
    "    the_abbreviations =\\\n",
    "        the_df['STANDARDIZEDNAME_1'].str\\\n",
    "                                    .extractall(\n",
    "                                        pat=the_regexs,\n",
    "                                        flags=flags)\n",
    "    print('constructed abbreviation list...')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "created interim abbreviations data frame...\ndone\n"
    }
   ],
   "source": [
    "# Since we've run on the entire dataset we can now\n",
    "# flatten the dataframe, de-duplicate and then directly label\n",
    "if os.path.exists(abbreviation_pickle):\n",
    "    len(the_abbreviations.all_caps.unique()) +\\\n",
    "        len(the_abbreviations.dot_abbreviation.unique()) # 1151\n",
    "\n",
    "    # now we need to get the count of unique abbreviations so that we can\n",
    "    # label in priority order. We also drop those abbreviations only occuring once\n",
    "    # since they have a 1 / 26,660 chance of occuring (not worth our effort)\n",
    "\n",
    "    # to properly label the all caps and abbreviations we need the \n",
    "    # context in which they occur. Since we're mapping to one definition\n",
    "    # we assume only the first instance is really needed and label off of that\n",
    "    abbreviations_to_label =\\\n",
    "        pd.concat(\n",
    "            (the_abbreviations.drop_duplicates(\n",
    "                subset=['all_caps'],\n",
    "                keep='first')['all_caps'],\n",
    "            the_abbreviations.drop_duplicates(\n",
    "                subset=['dot_abbreviation'],\n",
    "                keep='first')['dot_abbreviation']\n",
    "            ),\n",
    "            axis=0\n",
    "        ).dropna()\\\n",
    "        .droplevel('match')\\\n",
    "        .reset_index() # so that index is a column\n",
    "\n",
    "    abbreviations_to_label.rename(columns={'index':'the_df_index', 0:'abbreviation'}, inplace=True)\n",
    "    print('created interim abbreviations data frame...')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "created mapping columns for former labels and their expansions...\n"
    }
   ],
   "source": [
    "if os.path.exists(abbreviation_pickle):\n",
    "    # note here we read the main pickle file and assume \n",
    "    # those pickle files with random extensiosn were/are consolidated into this\n",
    "    # the other pickle files are named to prevent overwriting ongoing work\n",
    "    expanded_labels = pd.read_pickle(abbreviation_pickle)\n",
    "    last_labeled_index = expanded_labels.index(None)\n",
    "    former_labels = abbreviations_to_label.abbreviation[:last_labeled_index] #set(already_labeled[:last_labeled_index])\n",
    "\n",
    "    unseen_abbreviations =\\\n",
    "        abbreviations_to_label.query('abbreviation not in @former_labels')\n",
    "    unseen_abbreviations.abbreviation.value_counts()\n",
    "    # note I'm seeing 1 across the board, both when using not in and in\n",
    "    #   which suggests that we leave these be for now because their occurence\n",
    "    # is so rare out of 26,660, although the bulk may be significant; better\n",
    "    # to circle back though\n",
    "    print('created mapping columns for former labels and their expansions...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done/skipped manual labelling\n"
    }
   ],
   "source": [
    "#  So now we manually label them and dump them here\n",
    "#  This is the procedure we follow\n",
    "#       A) if a capitalized word is an entire word, leave it alone (no label)\n",
    "#       B) provide a label for all dotted abbreviated words\n",
    "if not SKIP_THIS:\n",
    "    def display_func(row):\n",
    "        # Note: We use globally available the_df to get context, bad form I know\n",
    "        display(\n",
    "            Markdown(\n",
    "                \"**Context:** \" +  the_df.loc[ row.the_df_index ].NAME_1 \\\n",
    "            +   \"\\n\\n\" + row.abbreviation\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def preprocessor(x, y):\n",
    "        # only take standardized column, leave everything else\n",
    "        return x.abbreviation, y\n",
    "\n",
    "    if os.path.exists(abbreviation_pickle) and not SKIP_THIS:\n",
    "        labelling_widget = ClassLabeller(\n",
    "            features=abbreviations_to_label,\n",
    "            model=pipeline,\n",
    "            model_preprocess=preprocessor,\n",
    "            display_func=display_func,\n",
    "            options=['No Label'],\n",
    "            acquisition_function='entropy'\n",
    "        )\n",
    "\n",
    "        labelling_widget\n",
    "print('done/skipped manual labelling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Every now and then, with the labels in hand we simply output them (if a file doesn't already exist)\n",
    "# so that we can save them incrementally. We should manually rename older files; this should\n",
    "# be basically a 1 time process.\n",
    "\n",
    "# Temp, save work locally so we don't loooooose it! \n",
    "if os.path.exists(abbreviation_pickle) and not SKIP_THIS:\n",
    "    import random\n",
    "    random_number = str(random.randint(0,255))\n",
    "    pickle.dump(expanded_labels,\n",
    "                open(abbreviation_pickle+random_number, 'wb'))\n",
    "    print(\"done/don't forget to consolidate abbreviations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# Now we do a mass search and replace on STANDARDIZED_NAME_1 and STANDRADIZED_NAME with the labels that we have\n",
    "\n",
    "# We follow this overflow thread\n",
    "# see: https://stackoverflow.com/a/48887382/3662899\n",
    "\n",
    "# First, construct an abbreviation to its expansion  dictionary\n",
    "label_mapper =\\\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"abbreviation\": \\\n",
    "abbreviations_to_label.abbreviation[:last_labeled_index].values,\n",
    "            \"expanded\": expanded_labels[:last_labeled_index]\n",
    "        }\n",
    "    )\n",
    "copy_over_index = (label_mapper.expanded == \"No Label\") | (label_mapper.expanded == \"Submit.\")\n",
    "label_mapper.expanded[copy_over_index] =\\\n",
    "    label_mapper.abbreviation[copy_over_index]\n",
    "\n",
    "# Add in one-off acronyms observed through labelling that need attention\n",
    "# too do, make this json or a python file\n",
    "one_off_mappings =(\n",
    "    ['AAS', 'Associate of Applied Science'],\n",
    "    ['ESL', 'English as a Second Language'],\n",
    "    ['Bus.Soft.', 'Business Software'],\n",
    "    ['App', 'Application'],\n",
    "    ['App.', 'Application'],\n",
    "    ['Dev.', 'Developer'],\n",
    "    ['CDL', 'Commerical Driver\\'s License'],\n",
    "    ['win', 'Windows'],\n",
    "    ['Win', 'Windows'],    \n",
    "    ['Contg.', 'Continuing'],\n",
    "    ['Ed.', 'Education'],\n",
    "    ['Mgmt.', 'Management'],\n",
    "    ['Mous', 'Microsoft Office User Specialist']\n",
    ")\n",
    "\n",
    "for mapping in one_off_mappings:\n",
    "    label_mapper.loc[len(label_mapper)+1] = mapping\n",
    "\n",
    "# normally we'd map through rep_dict but we want matches to occur\n",
    "# on word-like boundaries, space, /, and :, we use re.escape to properly escape\n",
    "\n",
    "# # Todo: the space regex forces matches at start of string to drop\n",
    "# #  need to make a regex that includes ^ starts\n",
    "# space = \" \"\n",
    "# slash = \"/\"\n",
    "# colon = \":\"\n",
    "# rep_dict = {\n",
    "#     **dict(zip(label_mapper.abbreviation+space, label_mapper.expanded+space)),\n",
    "#     **dict(zip(label_mapper.abbreviation+slash, label_mapper.expanded+slash)),\n",
    "#     **dict(zip(label_mapper.abbreviation+colon, label_mapper.expanded+colon))\n",
    "# }\n",
    "\n",
    "rep_dict = {\n",
    "    **dict(zip(label_mapper.abbreviation, label_mapper.expanded))\n",
    "}\n",
    "\n",
    "\n",
    "#pattern = re.compile(\"[\\b\\W]|\".join([re.escape(k) for k in rep_dict.keys()]), re.M)\n",
    "pattern = re.compile(\n",
    "    \"|\".join([re.escape(k) for k in rep_dict.keys()]),\n",
    "    re.M)\n",
    "\n",
    "def my_lookup(x):\n",
    "    if not rep_dict.get(x, False):\n",
    "        return rep_dict.get(x[1:], False)\n",
    "\n",
    "start_abbrev = re.compile(\n",
    "    \"^\"+\"|^\".join([re.escape(k) for k in rep_dict.keys()]), re.M)\n",
    "start_abbrev = re.compile(\n",
    "    \"^HVAC\", re.M)\n",
    "\n",
    "def multiple_replace(string):\n",
    "    return pattern.sub(lambda x: rep_dict[x.group(0)], string)\n",
    "\n",
    "def multiple_replace2(string):\n",
    "    return start_abbrev.sub(\n",
    "        lambda x: my_lookup(x), string)\n",
    "\n",
    "def term_grouped_regex(term=\"\", right_regex=\"\", left_regex=\"\"):\n",
    "    #return left_regex + '(' + re.escape(term) + ')' + right_regex\n",
    "    mystr = left_regex + '(' +\\\n",
    "                f\"?P<{term}>\"   +\\\n",
    "                re.escape(term) +\\\n",
    "            ')' +\\\n",
    "            right_regex\n",
    "    return mystr\n",
    "\n",
    "def make_term_grouped_regex(term=\"\", right_regex=\"\", left_regex=\"\"):\n",
    "    mystr = left_regex + '(' +\\\n",
    "                re.escape(term) +\\\n",
    "            ')' + right_regex\n",
    "    return mystr\n",
    "\n",
    "\n",
    "def make_grouped_regexes(replace_dict, left_regex=\"\", right_regex=\"\"):\n",
    "    return (make_term_grouped_regex(left_regex=left_regex,\n",
    "                                    term=key,\n",
    "                                    right_regex=right_regex)\\\n",
    "                                        for key in replace_dict.keys()\n",
    "            )\n",
    "\n",
    "# a_abbrev = re.compile(\n",
    "#     \"|\".join([\n",
    "#         make_term_grouped_regex(\n",
    "#             left_regex=\"^\",\n",
    "#             term=k\n",
    "#         ) for k in rep_dict.keys()]), re.M)\n",
    "    # \"|\".join(\n",
    "    #     make_grouped_regexes(rep_dict, left_regex=\"^\")\n",
    "    # ) +\\\n",
    "\n",
    "\n",
    "a_abbrev = re.compile(\n",
    "    \"|\".join(\n",
    "        make_grouped_regexes(rep_dict, left_regex=r'\\s', right_regex=r'\\s')\n",
    "    )\n",
    ")\n",
    "\n",
    "draft_output = the_df.iloc[:1000,:][['NAME_1']]\n",
    "pd.set_option('display.max_rows', None)\n",
    "# draft_output['MULTI_REPLACE_STANDARDIZEDNAME_1'] =\\\n",
    "#     draft_output['NAME_1'].map(multiple_replace3)\n",
    "# draft_output[['MULTI_REPLACE_STANDARDIZEDNAME_1', 'NAME_1']]\n",
    "\n",
    "the_content = re.compile(r'\\b(?P<key>\\w+)\\b')\n",
    "\n",
    "test_string = 'Straight Truck Driver - CDL B'\n",
    "test_string = \"Bus.Soft. App/Office Proc.Legal/\"\n",
    "\n",
    "def lookup_match(matchobj):\n",
    "    #  The match corresponds to a key with regexs \n",
    "    # surrounding it. To properly replace it we\n",
    "    # replace the key with its value in the whole matched\n",
    "    # string\n",
    "    the_original_string = matchobj.group(0)\n",
    "    print('in lookup match')\n",
    "\n",
    "    the_key = the_content.search(\n",
    "        matchobj.group(0)\n",
    "        ).group('key')\n",
    "\n",
    "    the_value = rep_dict.get(the_key, None)\n",
    "    if not the_value:\n",
    "        # try the whole thing\n",
    "        #print(rep_dict)\n",
    "        #print(the_original_string.strip())\n",
    "        the_value = rep_dict[the_original_string.strip()]\n",
    "        # we then use the entire string\n",
    "        print(the_key, \"|\", the_original_string)\n",
    "\n",
    "    print(the_key, \"|\", the_original_string)\n",
    "    #return the_original_string.replace(lookup_match, the_value)\n",
    "    return the_original_string.replace(the_key, the_value)\n",
    "\n",
    "\n",
    "def lookup_match(matchobj):\n",
    "    #  The match corresponds to a key with regexs \n",
    "    # surrounding it. To properly replace it we\n",
    "    # replace the key with its value inside of the whole matched\n",
    "    # string.\n",
    "\n",
    "    # This is overly complicated but we have a problem\n",
    "    # of ambiguity in that the keys \"Bus\" and \"Bus.Proc.\"\n",
    "    # will both match to \"Bus\" if you strip out possible\n",
    "    # left and right regexes; there is no way to know that\n",
    "    # the periods in \"Bus.Proc.\" don't indicate any character,\n",
    "    # e.g. \"BusXProcY\"\n",
    "    #\n",
    "    # So what we do is use the beg-for-forgiveness paradigm\n",
    "    # and first attempt to match to the large possible match (\"Bus.Proc.\")\n",
    "    # and then match to the content found by a specialized regex only\n",
    "    # if that fails\n",
    "    the_original_string = matchobj.group(0)\n",
    "\n",
    "    the_key = the_original_string.strip()\n",
    "    final_word = rep_dict.get(\n",
    "        the_key,\n",
    "        None\n",
    "    )\n",
    "\n",
    "    if not final_word:\n",
    "        shorter_key = the_content.search(\n",
    "            the_original_string\n",
    "            ).group('key')\n",
    "        final_word = rep_dict[shorter_key]\n",
    "        the_key = shorter_key\n",
    "\n",
    "    return the_original_string.replace(the_key, final_word)\n",
    "\n",
    "\n",
    "# See: https://stackoverflow.com/a/61952495/3662899\n",
    "# So, we can have more than one match in a given string, so we \n",
    "# need to \n",
    "\n",
    "# Here we have a bank of regexs for very specific left, rgith situations\n",
    "#   we could combine them for efficiency but it's easier to debug, examine\n",
    "#   in a special case row by row bank\n",
    "a_abbrev = re.compile(\n",
    "    \"|\".join(   # match words at start of string\n",
    "        make_grouped_regexes(rep_dict, left_regex=r'^', right_regex=r'\\s')\n",
    "    ) + \"|\" +\\\n",
    "    \"|\".join(   # match words surrounded by spaces\n",
    "        make_grouped_regexes(rep_dict, left_regex=r'\\s', right_regex=r'\\s')\n",
    "    ) + \"|\" +\\\n",
    "    \"|\".join(   # match words that make up entire fields, e.g. 'Nursing'\n",
    "        make_grouped_regexes(rep_dict, left_regex=r'^', right_regex=r'$')\n",
    "    ) + \"|\" +\\\n",
    "    \"|\".join(   # match words at end of string preceded by space or slash\n",
    "        make_grouped_regexes(rep_dict, left_regex=r'[\\s/]', right_regex=r'$')\n",
    "    ) + \"|\" +\\\n",
    "    \"|\".join(   # match words within string that follow a slash, end with a space or slash\n",
    "        make_grouped_regexes(rep_dict, left_regex=r'/', right_regex=r'[\\s/]')\n",
    "    )    \n",
    ")\n",
    "\n",
    "def multiple_replace(string):\n",
    "    return a_abbrev.sub(lookup_match, string)\n",
    "\n",
    "#re.sub(r'\\s'+'(?P<CDL>CDL)'+r'\\s' + '|' + \"(?P<woot>ABC)\", dashrepl, test_string)\n",
    "\n",
    "#print(a_abbrev)\n",
    "# test_string = \"A+/N+/Mous\"\n",
    "# test_string = \"/MOUS\"\n",
    "# test_string = \"Mous\"\n",
    "\n",
    "# k = re.compile(\"MOUS|^Mous$\")\n",
    "# print( test_string in label_mapper.abbreviation)\n",
    "# print( test_string in rep_dict, rep_dict[test_string])\n",
    "# print(\"|\"+label_mapper.abbreviation.iloc[-1]+'|')\n",
    "\n",
    "# print(\n",
    "#     k.sub(\"Microsoft Office User Something\", test_string)\n",
    "# )\n",
    "\n",
    "#print(a_abbrev.pattern[100:])\n",
    "\n",
    "# print(a_abbrev)\n",
    "# k =multiple_replace(test_string)\n",
    "# print(test_string)\n",
    "# print(k)\n",
    "\n",
    "draft_output['MULTI_REPLACE_STANDARDIZEDNAME_1'] =\\\n",
    "    draft_output['NAME_1'].map(multiple_replace)\n",
    "\n",
    "# so, see: https://stackoverflow.com/a/61952495/3662899\n",
    "# or we just run two times, again. Pretty simple, not scalable in the limit but whatever? in some sense it's probably about the same givne that we loop a constant\n",
    "# number of times and I treat the scan and replace as O(1)\n",
    "\n",
    "# Takes 15 seconds on my machine\n",
    "draft_output['MULTI_REPLACE_STANDARDIZEDNAME_1'] =\\\n",
    "    draft_output['MULTI_REPLACE_STANDARDIZEDNAME_1'].map(multiple_replace)\n",
    "draft_output['MULTI_REPLACE_STANDARDIZEDNAME_1'] =\\\n",
    "    draft_output['MULTI_REPLACE_STANDARDIZEDNAME_1'].map(multiple_replace)\n",
    "\n",
    "\n",
    "#draft_output[['MULTI_REPLACE_STANDARDIZEDNAME_1', 'NAME_1']]\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# Here we ingest Career One Stop certifications\n",
    "#   I was goign to use this to de-acroymn-ize mentions but now am unsure\n",
    "# if this is critical. It also may introduce errors, e.g. AES mapping to\n",
    "# the \"wrong acroymn\"\n",
    "\n",
    "# Here we read in a .sql directly as text and parse out the data.\n",
    "# I do this to avoid the need for a database, db drivers, etc. \n",
    "# That said, this represented some investment in constructing the right regexs\n",
    "if not SKIP_THIS:\n",
    "    path = rootpath + externalpath + 'career_one_stop/'\n",
    "    credential_sql = 'TEST-2-CERTIFICATIONS.sql' # '2-CERTIFICATIONS.sql'\n",
    "\n",
    "    with open(path + credential_sql) as sql:\n",
    "        my_string = sql.read()\n",
    "\n",
    "    header_names =\\\n",
    "        (\n",
    "            'CERT_ID', 'CERT_NAME', 'ORG_ID', 'TRAINING', 'EXPERIENCE', \n",
    "            'EITHER', 'EXAM', 'RENEWAL', 'CEU', 'REEXAM', \n",
    "            'CPD', 'CERT_ANY', 'URL', 'ACRONYM', 'NSSB_URL', \n",
    "            'CERT_URL', 'CERT_LAST_UPDATE', 'KEYWORD1', 'KEYWORD2', 'KEYWORD3', \n",
    "            'SUPPRESS', 'DATEADDED', 'COMMENTS', 'VERIFIED', 'UPDATEDBY', \n",
    "            'CERT_DESCRIPTION', 'DELETED', 'EXAM_DETAILS'\n",
    "        )\n",
    "\n",
    "    # Pandas assumes atomic python types when reading from records,\n",
    "    # See: https://github.com/pandas-dev/pandas/issues/9381, so we need to use\n",
    "    # Python types here\n",
    "    dtypes =\\\n",
    "        np.dtype(\n",
    "            \"str, str, float, float,\"\n",
    "            \"float, float, float, str,\"\n",
    "            \"float, float, float, float,\"\n",
    "            \"str, str, str, str,\"\n",
    "            \"str, str, str, str,\"\n",
    "            \"str, str, str, str,\" \n",
    "            \"str, str, float, str\"\n",
    "        )\n",
    "\n",
    "    flags = re.MULTILINE | re.DOTALL | re.VERBOSE\n",
    "    the_fields_regex =\\\n",
    "        \"\"\"\n",
    "        (?P<values>Values\\n\\s+\\()  # Start with the word Value <newline> (\n",
    "            (?P<fields>.*?)        #    Grab all the field content\n",
    "        (?P<end>\\);)               # ... which stops at the terminating paren, ;\n",
    "        \"\"\"\n",
    "\n",
    "    the_fields = re.compile(the_fields_regex, flags=flags)\n",
    "\n",
    "    a_field_regex =\\\n",
    "        \"\"\"\n",
    "        '(?P<string>.*?)'[,)]           # get a quoted string ending at comma or paran or\n",
    "        |(?P<date_time>TO_DATE\\(.*?\\))  # get the TO_DATE, parse out actual date later or\n",
    "        |(?P<num>\\d),                   # get numeric or\n",
    "        |(?P<null>NULL)                 # get NULL\n",
    "        \"\"\"\n",
    "\n",
    "    a_field = re.compile(a_field_regex, flags=flags)\n",
    "\n",
    "    require_field_numbers = [1] # should be 13\n",
    "\n",
    "    def yield_certification_records(sql_file=my_string, require_field_numbers=require_field_numbers):\n",
    "        # do we skip those w/o certain fields, like acronymns\n",
    "        temp_data = [0]*28\n",
    "        for match in the_fields.finditer(sql_file):\n",
    "            break_match = False\n",
    "\n",
    "            for index, field in enumerate(a_field.finditer( match.group('fields') )):\n",
    "                grp = None\n",
    "                for grp, value in field.groupdict().items():\n",
    "                    if value:\n",
    "                        # then we transform the string value into the appropriate type, given the group name\n",
    "                        if grp == 'date_time':\n",
    "                            #  There is a difference between https://regex101.com/r/yphUXY/1/\n",
    "                            # and what I see Python do here; if I don't capture the entire thing\n",
    "                            # it gets re-raised as another potential match, even if I use ?:, etc.\n",
    "                            value = value[9:28] # todo: convert to datetime\n",
    "                        if grp == 'null':\n",
    "                            value = None\n",
    "                            if index in require_field_numbers:\n",
    "                                break_match = True\n",
    "\n",
    "                        if grp == 'num':\n",
    "                            value = int(value)\n",
    "\n",
    "                        temp_data[index] = value\n",
    "                        break # only one possible match value\n",
    "                if break_match: # and don't look at other fields\n",
    "                    break\n",
    "\n",
    "            if not break_match:\n",
    "                yield tuple(value for value in temp_data)\n",
    "            \n",
    "            break_match = False\n",
    "\n",
    "    certification_df =\\\n",
    "        pd.DataFrame.from_records(\n",
    "            yield_certification_records(),\n",
    "            columns=header_names)\n",
    "    certification_df\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "18"
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "source": [
    "# 3) \n",
    "# Then go after odd static patterns that are common \n",
    "# ... A.A., AAS,e ends-with \"/\", etc etc\n",
    "# \"Applied Certificate in...\" <--- thing is, this could really be a program\n",
    "# the_df.STANDARDIZEDNAME_1 =\\\n",
    "#     the_df.STANDARDIZEDNAME_1.str.replace(\"A.A.\",\"\", case=False)\n",
    "\n",
    "# For efficiency purposes we handle repeats in the largest majority of cases that we've\n",
    "# observed as being caused by how peope use certification language;\n",
    "#\n",
    "# A lot of people say Oracle <Oracle Certification>, which can lead to repeats\n",
    "# My expansion of 'A' in CDL Class A expands A to include Class A since most write\n",
    "# CDL instead of CDL Class A\n",
    "check_for = [f\"({word} {word})\" for word in ['oracle', 'class']]\n",
    "has_repeats =\\\n",
    "    draft_output['MULTI_REPLACE_STANDARDIZEDNAME_1']\\\n",
    "          .str\\\n",
    "          .contains(\"|\".join(check_for),\n",
    "                    regex=True,\n",
    "                    flags=re.IGNORECASE)\n",
    "\n",
    "draft_output.loc[has_repeats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done The data is 100 long\n"
    }
   ],
   "source": [
    "# This is the evaluation part of the program and course name standardizations\n",
    "# along with the provider name. My goal is to have 85%+ standardized, send out\n",
    "# that 85% will come from the jefferey's interval\n",
    "\n",
    "# Evaluation Rubric:\n",
    "#   A) Here we label clearly wrong snippets, anything that is marginal we mark as\n",
    "# standardized for purposes of this evaluation because we want to err on the side\n",
    "# of giving overly specific information, which includes odd info\n",
    "#   B) We also click through quickly, not overly dwelling one any one example, the\n",
    "# goal here is to get the evaulation done quickly since it's so manual\n",
    "#   C) For now we ignore casingl there does need to be a camel casing applied to\n",
    "# all caps\n",
    "\n",
    "# We create a series of data to evaluate\n",
    "columns_to_check = ['MULTI_REPLACE_STANDARDIZEDNAME_1'] # we know NAME is mostly fine, 'STANDARDIZEDNAME']\n",
    "the_data =\\\n",
    "    np.concatenate(\n",
    "        (\n",
    "            draft_output[columns_to_check[0]].to_numpy(),\n",
    "            #the_df[columns_to_check[1]].to_numpy()\n",
    "        )\n",
    "    )\n",
    "    \n",
    "# we shuffle the data to elminate any bias across/within the columns when\n",
    "# evaluting\n",
    "random.Random(42).shuffle(the_data)\n",
    "print('done', f'The data is {len(the_data)} long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "ClassLabeller(children=(HBox(children=(HBox(children=(FloatProgress(value=0.0, description='Progress:', max=1.â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3021b56ccc224930848f7fd6c8694e28"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "def display_func(row):\n",
    "    \"\"\"\n",
    "    The display function gets passed your data - in the\n",
    "    case of a dataframe, it gets passed a row - and then\n",
    "    has to \"display\" your data in whatever way you want.\n",
    "\n",
    "    It doesn't need to return anything\n",
    "    \"\"\"\n",
    "    display(Markdown(row))\n",
    "    #display(Markdown(\"**At:** \" + row[\"timestamp\"]))\n",
    "\n",
    "def preprocessor(x, y):\n",
    "    # only take standardized column, leave everything else\n",
    "    return x, y\n",
    "\n",
    "verification_widget = ClassLabeller(\n",
    "    features=the_data,\n",
    "    model=pipeline,\n",
    "    model_preprocess=preprocessor,\n",
    "    display_func=display_func,\n",
    "    options=['standardized', 'not standardized'],\n",
    "    acquisition_function='margin'\n",
    ")\n",
    "\n",
    "verification_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "jeffreys bionomial proportion is: [0.95, 0.95]\nWe examined 100 labels, of which 95 are correct. There are 100 labels.\n"
    }
   ],
   "source": [
    "# insert bionomial proprtion esimator here\n",
    "\n",
    "def print_CI(labels, response_is_standardized = \"standardized\", method = \"jeffreys\"):\n",
    "    successful_count = sum(\n",
    "        response_is_standardized == label for label in labels\n",
    "    )\n",
    "    not_examined_count = sum(\n",
    "        None == label for label in labels\n",
    "    )\n",
    "\n",
    "    CI = proportion_confint(\n",
    "            count= successful_count,\n",
    "            nobs= len(labels) - not_examined_count,\n",
    "            alpha = 0.95,\n",
    "            method=method\n",
    "        )\n",
    "    print(f\"{method} bionomial proportion is: [{CI[0]:.2f}, {CI[1]:.2f}]\",\n",
    ")\n",
    "    print(f\"We examined {len(labels) - not_examined_count} labels, of which {successful_count} are correct. There are {len(labels)} labels.\")\n",
    "print_CI(labels=verification_widget.new_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# 4)\n",
    "# Now we write out the verfiied results\n",
    "# ... finally we can write this out as our first complete lookup table\n",
    "# for the NAME field\n",
    "write_out = the_df\n",
    "\n",
    "write_out['STANDARDIZEDNAME_1'] =\\\n",
    "    draft_output['MULTI_REPLACE_STANDARDIZEDNAME_1']\n",
    "\n",
    "# shuffe the rows to better remove temporal baises\n",
    "write_out =\\\n",
    "    the_df.sample(frac=1, random_state=42, axis=0).reset_index(drop=True)\n",
    "\n",
    "write_out.to_csv(rootpath + interimpath + content_is + \".csv\",\n",
    "                index = False,\n",
    "                chunksize = 10000,\n",
    "                columns=columns_to_save)\n",
    "\n",
    "write_out.to_excel(rootpath + processedpath + content_is + \".xls\",\n",
    "            sheet_name=\"Standardized NAME and NAME_1\",\n",
    "            index=False,\n",
    "            columns=columns_to_save)\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37264bitd4adstandardizationpipenvcac7d9f4a0864f29b6353caf0213501a",
   "display_name": "Python 3.7.2 64-bit ('d4ad_standardization': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}