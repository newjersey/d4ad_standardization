{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# Importing the libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import re\n",
    "import regex  # for better, more capbale regex api\n",
    "import os\n",
    "import zipfile\n",
    "import more_itertools\n",
    "from itertools import chain\n",
    "import datetime\n",
    "import time\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "# active labeler related\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import ComplementNB  # corrects for class imbalance, SGD is pretty good too\n",
    "from sklearn.pipeline import Pipeline\n",
    "from superintendent import ClassLabeller\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)  # so we can peak at data and spot verify\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='char', ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', ComplementNB()),\n",
    "])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# Set up columns to keep, fields, locations for writing\n",
    "rootpath = \"/hdd/work/d4ad_standardization/\"\n",
    "processedpath = \"D4AD_Standardization/data/processed/\"\n",
    "externalpath = \"D4AD_Standardization/data/external/\"\n",
    "interimpath = \"D4AD_Standardization/data/interim/\"\n",
    "\n",
    "content_is = \"standardized_descriptions_and_degree_funding_type\"\n",
    "\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "\n",
    "filepath = \"standardized_name_and_name1.csv\" # builds off of notebook 5 work\n",
    "\n",
    "columns = [\n",
    "    \"STANDARDIZEDNAME_1\",\n",
    "    \"STANDARDIZEDNAME\",\n",
    "    \"DESCRIPTION\",\n",
    "    \"FEATURESDESCRIPTION\",\n",
    "    \"NAME_1\",\n",
    "    \"NAME\",\n",
    "    \"PREREQUISITES\",\n",
    "    \"STREET1\",\n",
    "    \"CITY\",\n",
    "    \"STATE\",\n",
    "    \"ZIP\",\n",
    "    \"WEBSITE\",\n",
    "    \"COUNTY\",\n",
    "    \"NONGOVAPPROVAL\",\n",
    "    \"STATECOMMENTS\",\n",
    "    \"CIPCODE\",\n",
    "    \"PROVIDERID\",\n",
    "    \"APPROVINGAGENCYID\"\n",
    "]\n",
    "\n",
    "columns_to_save = ['STANDARDIZED_DESCRIPTION', 'STANDARDIZED_FEATURESDESCRIPTION'] + columns\n",
    "\n",
    "SKIP_THIS = True # helps me be able to run all and not worry about pulling things\n",
    "# I already know I have on disk\n",
    "\n",
    "#df = pd.read_excel(rootpath + interimpath + filepath, usecols=columns)\n",
    "df = pd.read_csv(rootpath + interimpath + filepath, usecols=columns)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', False)\n",
    "\n",
    "the_df = df #df.sample(n=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done1\n"
    }
   ],
   "source": [
    "# 2) Here we apply the abbreviation expansion to the\n",
    "# description columns. This code is repeated from the 5.0 notebook and should be externalized into ./src somewhere\n",
    "#\n",
    "# We first construct the abbreviation mapper\n",
    "#\n",
    "# We also store off a copy of the df for manipulation\n",
    "# this has older name fields, for informing on funding (WOIA) and degree type (?)\n",
    "# as well as the standardized fields so taht we can remove the extranous content still in it\n",
    "# Note: this is mixing responsibilites and should be seperated into a new notebook\n",
    "\n",
    "label_mapper = pd.read_csv(\n",
    "    rootpath + externalpath + \"label_mapper.csv\"\n",
    ")\n",
    "\n",
    "draft_output = the_df[['DESCRIPTION', 'FEATURESDESCRIPTION',\n",
    "                       'STANDARDIZEDNAME_1', 'STANDARDIZEDNAME',\n",
    "                       'NAME_1', 'NAME']]\n",
    "\n",
    "\n",
    "def make_term_grouped_regex(term=\"\", right_regex=\"\", left_regex=\"\"):\n",
    "    mystr = left_regex + '(' +\\\n",
    "                re.escape(term) +\\\n",
    "            ')' + right_regex\n",
    "    return mystr\n",
    "\n",
    "def make_grouped_regexes(replacement, left_regex=\"\", right_regex=\"\"):\n",
    "    return (make_term_grouped_regex(left_regex=left_regex,\n",
    "                                    term=key,\n",
    "                                    right_regex=right_regex)\\\n",
    "            for key in replacement.keys()\n",
    "    )\n",
    "\n",
    "def construct_map(label_mapper=label_mapper):\n",
    "    return {\n",
    "        **dict(zip(label_mapper.abbreviation, label_mapper.expanded))\n",
    "    }\n",
    "\n",
    "replacement_map = construct_map()\n",
    "\n",
    "abbrevation_pattern =\\\n",
    "    regex.compile(\n",
    "        \"(?p)\" +\n",
    "        \"|\".join(   # match words at start of string\n",
    "            make_grouped_regexes(replacement_map, left_regex=r'^', right_regex=r'[\\s:]')\n",
    "        ) + \"|\" +\\\n",
    "        \"|\".join(   # match words surrounded by spaces\n",
    "            make_grouped_regexes(replacement_map, left_regex=r'\\s', right_regex=r'\\s')\n",
    "        ) + \"|\" +\\\n",
    "        \"|\".join(   # match words that make up entire fields, e.g. 'Nursing'\n",
    "            make_grouped_regexes(replacement_map, left_regex=r'^', right_regex=r'$')\n",
    "        ) + \"|\" +\\\n",
    "        \"|\".join(   # match words at end of string preceded by space or slash\n",
    "            make_grouped_regexes(replacement_map, left_regex=r'[\\s/]', right_regex=r'$')\n",
    "        ) + \"|\" +\\\n",
    "        \"|\".join(   # match words within string that follow a slash, end with a space or slash\n",
    "            make_grouped_regexes(replacement_map, left_regex=r'/', right_regex=r'[\\s/]')\n",
    "        )\n",
    "    )\n",
    "\n",
    "def multiple_mapper(string):\n",
    "    return abbrevation_pattern.sub(\n",
    "        lambda x: \\\n",
    "        x.group().replace( # replace the found string\n",
    "            more_itertools.first_true(x.groups() # where the first matched group...\n",
    "        ),  replacement_map[more_itertools.first_true(x.groups())] # ... is replaced with the lookup\n",
    "    ), string)\n",
    "print('done1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Done! That took 0:00:00.399092 time\n"
    }
   ],
   "source": [
    "# ... with the abbreviation mapper in hand we now simply apply to both description columns\n",
    "# it takes about 2.5 minutes each to run through all rows for both descriptions.\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "if not SKIP_THIS:\n",
    "    draft_output['STANDARDIZED_DESCRIPTION'] =\\\n",
    "        draft_output['DESCRIPTION'].dropna().map(multiple_mapper)\n",
    "    draft_output['STANDARDIZED_FEATURESDESCRIPTION'] =\\\n",
    "        draft_output['FEATURESDESCRIPTION'].dropna().map(multiple_mapper)\n",
    "else:\n",
    "    joining_columns = ['NAME_1', 'NAME']\n",
    "    interim_csv = \"standardized_descriptions_and_degree_funding_type.csv\"\n",
    "    already_standardized_descriptions =\\\n",
    "        pd.read_csv(rootpath+interimpath+interim_csv,\n",
    "        usecols=[\n",
    "            'STANDARDIZED_DESCRIPTION', \n",
    "            'STANDARDIZED_FEATURESDESCRIPTION'] + joining_columns)\\\n",
    "                .drop_duplicates(subset=joining_columns)  # not sure how or why we have dupes\n",
    "    # see: https://stackoverflow.com/questions/22720739/pandas-left-outer-join-results-in-table-larger-than-left-table\n",
    "    \n",
    "    read_in = draft_output.merge(\n",
    "            already_standardized_descriptions,\n",
    "            how='left',\n",
    "            on=joining_columns,\n",
    "            validate=\"m:1\"\n",
    "    )\n",
    "\n",
    "    assert len(read_in) == len(draft_output), f\"read in shape {len(read_in)} does not equal draft df {len(draft_output)}!\"\n",
    "    draft_output = read_in\n",
    "    \n",
    "end = datetime.datetime.now()\n",
    "print(f\"Done! That took {(end-start)} time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# 3) \n",
    "# Now we have to extract course funding type from the older\n",
    "# columns. \n",
    "\n",
    "woia_like =\\\n",
    "    regex.compile(\n",
    "        '''\n",
    "         (title\\s+[I|II|III|IV|1|2|3]+\\s)   # WOIA has 4 titles of funding in law\n",
    "        |(woia){d<=1}                       # is called WOIA, WIA, allowed to miss a letter\n",
    "        ''',\n",
    "        flags=regex.I|regex.VERBOSE)\n",
    "\n",
    "name =\\\n",
    "    draft_output['NAME'].dropna()\\\n",
    "                        .map(woia_like.search)\\\n",
    "                        .dropna().index\n",
    "\n",
    "name_1 =\\\n",
    "    draft_output['NAME_1'].dropna()\\\n",
    "                        .map(woia_like.search)\\\n",
    "                        .dropna().index\n",
    "\n",
    "descriptions =\\\n",
    "    draft_output['DESCRIPTION'].dropna()\\\n",
    "                          .map(woia_like.search)\\\n",
    "                          .dropna().index\n",
    "\n",
    "features_description =\\\n",
    "    draft_output['FEATURESDESCRIPTION'].dropna()\\\n",
    "                          .map(woia_like.search)\\\n",
    "                          .dropna().index\n",
    "\n",
    "woia_indices = name.union(name_1)\\\n",
    "                   .union(descriptions)\\\n",
    "                   .union(features_description)\n",
    "draft_output['IS_WOIA'] = False\n",
    "draft_output.loc[woia_indices, 'IS_WOIA'] = True\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "... there are 199 associate indices\n"
    }
   ],
   "source": [
    "# ... Finally we extact the degree type from the older columns, repeating the\n",
    "# procedure above but with slightly different regexes\n",
    "\n",
    "as_like =\\\n",
    "    regex.compile(\n",
    "        '''\n",
    "        [\\b\\s](A\\.A\\.S\\.)[\\b\\s]\n",
    "        |[\\b\\s](A\\.S\\.)[\\b\\s]\n",
    "        |[\\b\\s](AS\\sDe)                   # AS Degree\n",
    "        |[\\b\\s](AS\\sSc)                   # AS Science\n",
    "        |[\\b\\s](AAS)[\\b\\s]                 # applied associates of science\n",
    "        ''',\n",
    "        flags=regex.VERBOSE)\n",
    "\n",
    "\n",
    "name =\\\n",
    "    draft_output['NAME'].dropna()\\\n",
    "                        .map(as_like.search)\\\n",
    "                        .dropna().index\n",
    "\n",
    "name_1 =\\\n",
    "    draft_output['NAME_1'].dropna()\\\n",
    "                          .map(as_like.search)\\\n",
    "                          .dropna().index\n",
    "\n",
    "descriptions =\\\n",
    "    draft_output['DESCRIPTION'].dropna()\\\n",
    "                          .map(as_like.search)\\\n",
    "                          .dropna().index\n",
    "\n",
    "features_description =\\\n",
    "    draft_output['FEATURESDESCRIPTION'].dropna()\\\n",
    "                          .map(as_like.search)\\\n",
    "                          .dropna().index\n",
    "\n",
    "as_indices = name.union(name_1)\\\n",
    "                  .union(descriptions)\\\n",
    "                  .union(features_description)\n",
    "                   \n",
    "draft_output.loc[as_indices, 'Mentioned_Associates'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we go back for mentions of certificate and assign those\n",
    "cert_like =\\\n",
    "    regex.compile(\n",
    "        '''\n",
    "        (certification)\n",
    "        |(certificate)\n",
    "        |[\\s\\b](cert)[\\s\\b]\n",
    "        ''',\n",
    "        flags=regex.I|regex.VERBOSE)\n",
    "\n",
    "name =\\\n",
    "    draft_output['NAME'].dropna()\\\n",
    "                        .map(cert_like.search)\\\n",
    "                        .dropna().index\n",
    "\n",
    "name_1 =\\\n",
    "    draft_output['NAME_1'].dropna()\\\n",
    "                          .map(cert_like.search)\\\n",
    "                          .dropna().index\n",
    "\n",
    "descriptions =\\\n",
    "    draft_output['DESCRIPTION'].dropna()\\\n",
    "                          .map(cert_like.search)\\\n",
    "                          .dropna().index\n",
    "\n",
    "features_description =\\\n",
    "    draft_output['FEATURESDESCRIPTION'].dropna()\\\n",
    "                          .map(cert_like.search)\\\n",
    "                          .dropna().index\n",
    "\n",
    "cert_indices = name.union(name_1)\\\n",
    "                   .union(descriptions)\\\n",
    "                   .union(features_description)\n",
    "\n",
    "draft_output.loc[cert_indices, 'Mentioned_Certificate'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) \n",
    "# Now we do some simple removals for known degree related mentions in the name fields\n",
    "\n",
    "degree_cert_variants =\\\n",
    "    [\"A.S.\",\n",
    "     \"AAS Degree\",\n",
    "     \"AAS -\",\n",
    "     \"A.S. Degree\",\n",
    "     \"AS Degree\",     \n",
    "     \"Degree\",\n",
    "     \"degree\",\n",
    "     \"certificate\",\n",
    "     \"Certificate\",\n",
    "     \"Associate of Applied Science\",\n",
    "     \"-[\\s\\b]Associate\",\n",
    "     \"^\\s*In\"]\n",
    "\n",
    "draft_output['CLEANED_STANDARDIZED_NAME_1'] =\\\n",
    "    draft_output['STANDARDIZEDNAME_1'].replace(degree_cert_variants, \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done The stratified validation data is 65 long\n"
    }
   ],
   "source": [
    "# This is the evaluation part of the program and course name standardizations\n",
    "# along with the provider name. My goal is to have 85%+ standardized, send out\n",
    "# that 85% will come from the jefferey's interval\n",
    "\n",
    "# Evaluation Rubric:\n",
    "#   A) Here we label clearly wrong snippets, anything that is marginal we mark as\n",
    "# standardized for purposes of this evaluation because we want to err on the side\n",
    "# of giving overly specific information, which includes odd info\n",
    "#   B) We also click through quickly, not overly dwelling one any one example, the\n",
    "# goal here is to get the evaulation done quickly since it's so manual\n",
    "#   C) For now we ignore casingl there does need to be a camel casing applied to\n",
    "# all caps\n",
    "\n",
    "def stratified_sample(the_data, strata, size):\n",
    "    some_frac = size/len(the_data)\n",
    "    return \\\n",
    "        the_data.groupby(\n",
    "            strata\n",
    "        ).apply(\n",
    "            lambda g: g.sample(\n",
    "                frac=1\n",
    "                )\n",
    "        )\n",
    "\n",
    "key_factors_to_consider = ['IS_WOIA', 'Mentioned_Certificate', 'Mentioned_Associates']\n",
    "\n",
    "# We create a series of data to evaluate\n",
    "columns_to_check = ['CLEANED_STANDARDIZED_NAME_1', 'IS_WOIA',\n",
    "                    'Mentioned_Certificate', 'Mentioned_Associates',\n",
    "                    'STANDARDIZED_DESCRIPTION', 'STANDARDIZED_FEATURESDESCRIPTION']\n",
    "\n",
    "check_this_many = 100 * len(columns_to_check) # we mark if ANY column are wrong\n",
    "# the_data = draft_output.sample(check_this_many,random_state=42)\\\n",
    "#                        .loc[:, columns_to_check]\n",
    "the_data = stratified_sample(draft_output, strata=key_factors_to_consider, size=check_this_many)\n",
    "\n",
    "# we shuffle the data to elminate any bias across/within the columns when\n",
    "# evaluting\n",
    "print('done', f'The stratified validation data is {len(the_data)} long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "ClassLabeller(children=(HBox(children=(HBox(children=(FloatProgress(value=0.0, description='Progress:', max=1.â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85006fd3b0d146f0bbf8a4c7c12e21f6"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "markdown = []\n",
    "\n",
    "def display_func(row):\n",
    "    \"\"\"\n",
    "    The display function gets passed your data - in the\n",
    "    case of a dataframe, it gets passed a row - and then\n",
    "    has to \"display\" your data in whatever way you want.\n",
    "\n",
    "    It doesn't need to return anything\n",
    "    \"\"\"\n",
    "\n",
    "    the_string =\\\n",
    "            \"**IS_WOIA:** \" + str(row[\"IS_WOIA\"]) +\\\n",
    "            \" **Cert:** \" + str(row[\"Mentioned_Certificate\"]) +\\\n",
    "            \" **Assoc:** \" + str(row[\"Mentioned_Associates\"]) +\\\n",
    "            \"\\n\\n**Provider:** \" + str(row[\"STANDARDIZEDNAME\"]) + \"\" +\\\n",
    "            \"\\n\\n**Course Name:** \" + str(row[\"CLEANED_STANDARDIZED_NAME_1\"]) + \"\" +\\\n",
    "            \"\\n\\n**Description:** \" + str(row[\"STANDARDIZED_DESCRIPTION\"]) + \"\" +\\\n",
    "            \"\\n\\n**Featured Description:** \" + str(row[\"STANDARDIZED_FEATURESDESCRIPTION\"]) + \"\" +\\\n",
    "            \"\\n\\n**(unstandardized):** [Name_1] \" + str(row[\"NAME_1\"]) + \" [Name] \" + str(row[\"NAME\"])\n",
    "\n",
    "\n",
    "    markdown_string =\\\n",
    "        Markdown(the_string)\n",
    "\n",
    "    display(\n",
    "        markdown_string\n",
    "    )\n",
    "    markdown.append(the_string)\n",
    "\n",
    "def preprocessor(x, y):\n",
    "    # only take standardized column, leave everything else\n",
    "    return x, y\n",
    "\n",
    "verification_widget = ClassLabeller(\n",
    "    features=the_data,\n",
    "    model=pipeline,\n",
    "    model_preprocess=preprocessor,\n",
    "    display_func=display_func,\n",
    "    options=['standardized', 'not standardized'],\n",
    "    acquisition_function='margin'\n",
    ")\n",
    "\n",
    "verification_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "**IS_WOIA:** False **Cert:** True **Assoc:** True\n\n**Provider:** Mercer County Community College\n\n**Course Name:** Advertising and Graphic Design\n\n**Description:** The Advertising and Graphic Design option of the Visual Arts Associate of Applied Science degree prepares students for positions as designers, graphic communicators, and assistant art directors.  It also prepares students for advanced study in graphic design, advertising design, web design, or visual communication.  Successful graduates of the option will be able to:  apply computer applications to design principles;  visualize and practice professional typography;  recognize elements of proper design in professional-quality work;  design professional-quality logos, newsletters, posters, brochures, websites, publications and advertisements;  create web pages that use design principles that communicate effectively;  develop and present ideas in both written and oral formats;  create a professional portfolio to serve in the pursuit of further education or employment.\n\n**Featured Description:** Most coursework takes place in a studio using regularly upgraded professional quality hardware and software.  Applying Macintosh as well as Personal Computer platforms, the equipment used at MCCC is the same as that most commonly used by agencies, studios, and corporate art departments.  The program may be pursued on a full-time or part-time basis.  Some courses may only be offered during the day.\n\n**(unstandardized):** [Name_1] Advertising and Graphic Design [Name] Mercer County Community College - Certificate Programs (Non-Credit)\n"
    }
   ],
   "source": [
    "print(markdown[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "jeffreys bionomial proportion is: [0.95, 0.95]\nWe examined 100 labels, of which 95 are correct. There are 100 labels.\n"
    }
   ],
   "source": [
    "# insert bionomial proprtion esimator here\n",
    "\n",
    "def print_CI(labels, response_is_standardized = \"standardized\", method = \"jeffreys\"):\n",
    "    successful_count = sum(\n",
    "        response_is_standardized == label for label in labels\n",
    "    )\n",
    "    not_examined_count = sum(\n",
    "        None == label for label in labels\n",
    "    )\n",
    "\n",
    "    CI = proportion_confint(\n",
    "            count= successful_count,\n",
    "            nobs= len(labels) - not_examined_count,\n",
    "            alpha = 0.95,\n",
    "            method=method\n",
    "        )\n",
    "    print(f\"{method} bionomial proportion is: [{CI[0]:.2f}, {CI[1]:.2f}]\",\n",
    ")\n",
    "    print(f\"We examined {len(labels) - not_examined_count} labels, of which {successful_count} are correct. There are {len(labels)} labels.\")\n",
    "print_CI(labels=verification_widget.new_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Index(['IS_WOIA', 'Degree_Type', 'STANDARDIZED_DESCRIPTION',\n       'STANDARDIZED_FEATURESDESCRIPTION', 'CLEANED_STANDARDIZED_NAME_1',\n       'STANDARDIZEDNAME', 'STANDARDIZEDNAME_1', 'DESCRIPTION',\n       'FEATURESDESCRIPTION', 'NAME_1', 'NAME'],\n      dtype='object')\ndone\n"
    }
   ],
   "source": [
    "# 4)\n",
    "# Now we write out the verfiied results\n",
    "# ... finally we can write this out as our first complete lookup table\n",
    "# for the NAME field\n",
    "write_out = draft_output[\n",
    "    [\n",
    "        'IS_WOIA', 'Degree_Type',\n",
    "        'STANDARDIZED_DESCRIPTION', 'STANDARDIZED_FEATURESDESCRIPTION', \n",
    "        'CLEANED_STANDARDIZED_NAME_1', 'STANDARDIZEDNAME',\n",
    "        'STANDARDIZEDNAME_1', 'DESCRIPTION',\n",
    "        'FEATURESDESCRIPTION', 'NAME_1', 'NAME'\n",
    "    ]\n",
    "]\n",
    "\n",
    "print(\n",
    "    \"We're writing ...\",\n",
    "    write_out.columns\n",
    ")\n",
    "\n",
    "# shuffe the rows to better remove temporal baises\n",
    "write_out =\\\n",
    "    write_out.sample(frac=1, random_state=42, axis=0).reset_index(drop=True)\n",
    "\n",
    "write_out.to_csv(rootpath + interimpath + content_is + \".csv\",\n",
    "                index = False,\n",
    "                chunksize = 10000)\n",
    "\n",
    "write_out.to_excel(rootpath + processedpath + content_is + \".xls\",\n",
    "            sheet_name=\"Standardized Descriptions\",\n",
    "            index=False)\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37264bitd4adstandardizationpipenvcac7d9f4a0864f29b6353caf0213501a",
   "display_name": "Python 3.7.2 64-bit ('d4ad_standardization': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}