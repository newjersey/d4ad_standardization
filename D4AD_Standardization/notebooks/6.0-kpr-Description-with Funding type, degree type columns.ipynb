{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# Importing the libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import re\n",
    "import regex  # for better, more capbale regex api\n",
    "import os\n",
    "import zipfile\n",
    "import more_itertools\n",
    "from itertools import chain\n",
    "import datetime\n",
    "import time\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "# active labeler related\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import ComplementNB  # corrects for class imbalance, SGD is pretty good too\n",
    "from sklearn.pipeline import Pipeline\n",
    "from superintendent import ClassLabeller\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)  # so we can peak at data and spot verify\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='char', ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', ComplementNB()),\n",
    "])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# Set up columns to keep, fields, locations for writing\n",
    "rootpath = \"/hdd/work/d4ad_standardization/\"\n",
    "processedpath = \"D4AD_Standardization/data/processed/\"\n",
    "externalpath = \"D4AD_Standardization/data/external/\"\n",
    "interimpath = \"D4AD_Standardization/data/interim/\"\n",
    "\n",
    "content_is = \"standardized_descriptions_and_degree_funding_type\"\n",
    "\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "\n",
    "filepath = \"standardized_name_and_name1.csv\" # builds off of notebook 5 work\n",
    "\n",
    "columns = [\n",
    "    \"STANDARDIZEDNAME_1\",\n",
    "    \"STANDARDIZEDNAME\",\n",
    "    \"DESCRIPTION\",\n",
    "    \"FEATURESDESCRIPTION\",\n",
    "    \"NAME_1\",\n",
    "    \"NAME\",\n",
    "    \"PREREQUISITES\",\n",
    "    \"STREET1\",\n",
    "    \"CITY\",\n",
    "    \"STATE\",\n",
    "    \"ZIP\",\n",
    "    \"WEBSITE\",\n",
    "    \"COUNTY\",\n",
    "    \"NONGOVAPPROVAL\",\n",
    "    \"STATECOMMENTS\",\n",
    "    \"CIPCODE\",\n",
    "    \"PROVIDERID\",\n",
    "    \"APPROVINGAGENCYID\"\n",
    "]\n",
    "\n",
    "columns_to_save = ['STANDARDIZED_DESCRIPTION', 'STANDARDIZED_FEATURESDESCRIPTION'] + columns\n",
    "\n",
    "SKIP_THIS = True # helps me be able to run all and not worry about pulling things\n",
    "# I already know I have on disk\n",
    "\n",
    "#df = pd.read_excel(rootpath + interimpath + filepath, usecols=columns)\n",
    "df = pd.read_csv(rootpath + interimpath + filepath, usecols=columns)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', False)\n",
    "\n",
    "the_df = df #df.sample(n=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done1\n"
    }
   ],
   "source": [
    "# 2) Here we apply the abbreviation expansion to the\n",
    "# description columns. This code is repeated from the 5.0 notebook and should be externalized into ./src somewhere\n",
    "#\n",
    "# We first construct the abbreviation mapper\n",
    "#\n",
    "# We also store off a copy of the df for manipulation\n",
    "# this has older name fields, for informing on funding (WOIA) and degree type (?)\n",
    "# as well as the standardized fields so taht we can remove the extranous content still in it\n",
    "# Note: this is mixing responsibilites and should be seperated into a new notebook\n",
    "\n",
    "label_mapper = pd.read_csv(\n",
    "    rootpath + externalpath + \"label_mapper.csv\"\n",
    ")\n",
    "\n",
    "draft_output = the_df[['DESCRIPTION', 'FEATURESDESCRIPTION',\n",
    "                       'STANDARDIZEDNAME_1', 'STANDARDIZEDNAME',\n",
    "                       'NAME_1', 'NAME']]\n",
    "\n",
    "\n",
    "def make_term_grouped_regex(term=\"\", right_regex=\"\", left_regex=\"\"):\n",
    "    mystr = left_regex + '(' +\\\n",
    "                re.escape(term) +\\\n",
    "            ')' + right_regex\n",
    "    return mystr\n",
    "\n",
    "def make_grouped_regexes(replacement, left_regex=\"\", right_regex=\"\"):\n",
    "    return (make_term_grouped_regex(left_regex=left_regex,\n",
    "                                    term=key,\n",
    "                                    right_regex=right_regex)\\\n",
    "            for key in replacement.keys()\n",
    "    )\n",
    "\n",
    "def construct_map(label_mapper=label_mapper):\n",
    "    return {\n",
    "        **dict(zip(label_mapper.abbreviation, label_mapper.expanded))\n",
    "    }\n",
    "\n",
    "replacement_map = construct_map()\n",
    "\n",
    "abbrevation_pattern =\\\n",
    "    regex.compile(\n",
    "        \"(?p)\" +\n",
    "        \"|\".join(   # match words at start of string\n",
    "            make_grouped_regexes(replacement_map, left_regex=r'^', right_regex=r'[\\s:]')\n",
    "        ) + \"|\" +\\\n",
    "        \"|\".join(   # match words surrounded by spaces\n",
    "            make_grouped_regexes(replacement_map, left_regex=r'\\s', right_regex=r'\\s')\n",
    "        ) + \"|\" +\\\n",
    "        \"|\".join(   # match words that make up entire fields, e.g. 'Nursing'\n",
    "            make_grouped_regexes(replacement_map, left_regex=r'^', right_regex=r'$')\n",
    "        ) + \"|\" +\\\n",
    "        \"|\".join(   # match words at end of string preceded by space or slash\n",
    "            make_grouped_regexes(replacement_map, left_regex=r'[\\s/]', right_regex=r'$')\n",
    "        ) + \"|\" +\\\n",
    "        \"|\".join(   # match words within string that follow a slash, end with a space or slash\n",
    "            make_grouped_regexes(replacement_map, left_regex=r'/', right_regex=r'[\\s/]')\n",
    "        )\n",
    "    )\n",
    "\n",
    "def multiple_mapper(string):\n",
    "    return abbrevation_pattern.sub(\n",
    "        lambda x: \\\n",
    "        x.group().replace( # replace the found string\n",
    "            more_itertools.first_true(x.groups() # where the first matched group...\n",
    "        ),  replacement_map[more_itertools.first_true(x.groups())] # ... is replaced with the lookup\n",
    "    ), string)\n",
    "print('done1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Done! That took 0:02:34.527864 time\n"
    }
   ],
   "source": [
    "# ... with the abbreviation mapper in hand we now simply apply to both description columns\n",
    "# it takes about 2.5 minutes each to run through all rows for both descriptions.\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "draft_output['STANDARDIZED_DESCRIPTION'] =\\\n",
    "    draft_output['DESCRIPTION'].dropna().map(multiple_mapper)\n",
    "draft_output['STANDARDIZED_FEATURESDESCRIPTION'] =\\\n",
    "    draft_output['FEATURESDESCRIPTION'].dropna().map(multiple_mapper)\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(f\"Done! That took {(end-start)} time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# 3) \n",
    "# Now we have to extract course funding type from the older\n",
    "# columns. \n",
    "\n",
    "woia_like =\\\n",
    "    regex.compile(\n",
    "        '''\n",
    "         (title\\s+[I|II|III|IV|1|2|3]+\\s)   # WOIA has 4 titles of funding in law\n",
    "        |(woia){d<=1}                       # is called WOIA, WIA, allowed to miss a letter\n",
    "        ''',\n",
    "        flags=regex.I|regex.VERBOSE)\n",
    "\n",
    "name =\\\n",
    "    draft_output['NAME'].dropna()\\\n",
    "                        .map(woia_like.search)\\\n",
    "                        .dropna().index\n",
    "\n",
    "name_1 =\\\n",
    "    draft_output['NAME_1'].dropna()\\\n",
    "                          .map(woia_like.search)\\\n",
    "                          .dropna().index\n",
    "\n",
    "descriptions =\\\n",
    "    draft_output['DESCRIPTION'].dropna()\\\n",
    "                          .map(woia_like.search)\\\n",
    "                          .dropna().index\n",
    "\n",
    "features_description =\\\n",
    "    draft_output['FEATURESDESCRIPTION'].dropna()\\\n",
    "                          .map(woia_like.search)\\\n",
    "                          .dropna().index\n",
    "\n",
    "woia_indices = name.union(name_1)\\\n",
    "                   .union(descriptions)\\\n",
    "                   .union(features_description)\n",
    "draft_output['IS_WOIA'] = False\n",
    "draft_output.loc[woia_indices, 'IS_WOIA'] = True\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... Finally we extact the degree type from the older columns, repeating the\n",
    "# procedure above but with slightly different regexes\n",
    "\n",
    "aas_like =\\\n",
    "    regex.compile(\n",
    "        '''\n",
    "        (AAS)                           # applied associates of science\n",
    "        |(applied.*associate.*science.*[.\\b])     # sentence containing applied science \n",
    "        ''',\n",
    "        flags=regex.I|regex.VERBOSE)\n",
    "\n",
    "name =\\\n",
    "    draft_output['NAME'].dropna()\\\n",
    "                        .map(aas_like.search)\\\n",
    "                        .dropna().index\n",
    "\n",
    "name_1 =\\\n",
    "    draft_output['NAME_1'].dropna()\\\n",
    "                          .map(aas_like.search)\\\n",
    "                          .dropna().index\n",
    "\n",
    "descriptions =\\\n",
    "    draft_output['DESCRIPTION'].dropna()\\\n",
    "                          .map(aas_like.search)\\\n",
    "                          .dropna().index\n",
    "\n",
    "features_description =\\\n",
    "    draft_output['FEATURESDESCRIPTION'].dropna()\\\n",
    "                          .map(aas_like.search)\\\n",
    "                          .dropna().index\n",
    "\n",
    "aas_indices = name.union(name_1)\\\n",
    "                   .union(descriptions)\\\n",
    "                   .union(features_description)\n",
    "draft_output.loc[aas_indices, 'Degree_Type'] = 'Applied Associates of Science'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done The data is 100 long\n"
    }
   ],
   "source": [
    "# This is the evaluation part of the program and course name standardizations\n",
    "# along with the provider name. My goal is to have 85%+ standardized, send out\n",
    "# that 85% will come from the jefferey's interval\n",
    "\n",
    "# Evaluation Rubric:\n",
    "#   A) Here we label clearly wrong snippets, anything that is marginal we mark as\n",
    "# standardized for purposes of this evaluation because we want to err on the side\n",
    "# of giving overly specific information, which includes odd info\n",
    "#   B) We also click through quickly, not overly dwelling one any one example, the\n",
    "# goal here is to get the evaulation done quickly since it's so manual\n",
    "#   C) For now we ignore casingl there does need to be a camel casing applied to\n",
    "# all caps\n",
    "\n",
    "# We create a series of data to evaluate\n",
    "columns_to_check = ['MULTI_REPLACE_STANDARDIZEDNAME_1'] # we know NAME is mostly fine, 'STANDARDIZEDNAME']\n",
    "the_data =\\\n",
    "    np.concatenate(\n",
    "        (\n",
    "            draft_output[columns_to_check[0]].to_numpy(),\n",
    "            #the_df[columns_to_check[1]].to_numpy()\n",
    "        )\n",
    "    )\n",
    "    \n",
    "# we shuffle the data to elminate any bias across/within the columns when\n",
    "# evaluting\n",
    "random.Random(42).shuffle(the_data)\n",
    "print('done', f'The data is {len(the_data)} long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "ClassLabeller(children=(HBox(children=(HBox(children=(FloatProgress(value=0.0, description='Progress:', max=1.â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3021b56ccc224930848f7fd6c8694e28"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "def display_func(row):\n",
    "    \"\"\"\n",
    "    The display function gets passed your data - in the\n",
    "    case of a dataframe, it gets passed a row - and then\n",
    "    has to \"display\" your data in whatever way you want.\n",
    "\n",
    "    It doesn't need to return anything\n",
    "    \"\"\"\n",
    "    display(Markdown(row))\n",
    "    #display(Markdown(\"**At:** \" + row[\"timestamp\"]))\n",
    "\n",
    "def preprocessor(x, y):\n",
    "    # only take standardized column, leave everything else\n",
    "    return x, y\n",
    "\n",
    "verification_widget = ClassLabeller(\n",
    "    features=the_data,\n",
    "    model=pipeline,\n",
    "    model_preprocess=preprocessor,\n",
    "    display_func=display_func,\n",
    "    options=['standardized', 'not standardized'],\n",
    "    acquisition_function='margin'\n",
    ")\n",
    "\n",
    "verification_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "jeffreys bionomial proportion is: [0.95, 0.95]\nWe examined 100 labels, of which 95 are correct. There are 100 labels.\n"
    }
   ],
   "source": [
    "# insert bionomial proprtion esimator here\n",
    "\n",
    "def print_CI(labels, response_is_standardized = \"standardized\", method = \"jeffreys\"):\n",
    "    successful_count = sum(\n",
    "        response_is_standardized == label for label in labels\n",
    "    )\n",
    "    not_examined_count = sum(\n",
    "        None == label for label in labels\n",
    "    )\n",
    "\n",
    "    CI = proportion_confint(\n",
    "            count= successful_count,\n",
    "            nobs= len(labels) - not_examined_count,\n",
    "            alpha = 0.95,\n",
    "            method=method\n",
    "        )\n",
    "    print(f\"{method} bionomial proportion is: [{CI[0]:.2f}, {CI[1]:.2f}]\",\n",
    ")\n",
    "    print(f\"We examined {len(labels) - not_examined_count} labels, of which {successful_count} are correct. There are {len(labels)} labels.\")\n",
    "print_CI(labels=verification_widget.new_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Index(['STANDARDIZED_DESCRIPTION', 'STANDARDIZED_FEATURESDESCRIPTION',\n       'STANDARDIZEDNAME_1', 'STANDARDIZEDNAME', 'DESCRIPTION',\n       'FEATURESDESCRIPTION', 'NAME_1', 'NAME', 'IS_WOIA', 'Degree_Type'],\n      dtype='object')\ndone\n"
    }
   ],
   "source": [
    "# 4)\n",
    "# Now we write out the verfiied results\n",
    "# ... finally we can write this out as our first complete lookup table\n",
    "# for the NAME field\n",
    "write_out = draft_output[\n",
    "    [\n",
    "        'STANDARDIZED_DESCRIPTION', 'STANDARDIZED_FEATURESDESCRIPTION', \n",
    "        'STANDARDIZEDNAME_1', 'STANDARDIZEDNAME', \n",
    "        'DESCRIPTION', 'FEATURESDESCRIPTION',\n",
    "        'NAME_1', 'NAME',\n",
    "        'IS_WOIA', 'Degree_Type'\n",
    "    ]\n",
    "]\n",
    "\n",
    "print(\n",
    "    write_out.columns\n",
    ")\n",
    "\n",
    "# shuffe the rows to better remove temporal baises\n",
    "write_out =\\\n",
    "    write_out.sample(frac=1, random_state=42, axis=0).reset_index(drop=True)\n",
    "\n",
    "write_out.to_csv(rootpath + interimpath + content_is + \".csv\",\n",
    "                index = False,\n",
    "                chunksize = 10000)\n",
    "\n",
    "write_out.to_excel(rootpath + processedpath + content_is + \".xls\",\n",
    "            sheet_name=\"Standardized Descriptions\",\n",
    "            index=False)\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37264bitd4adstandardizationpipenvcac7d9f4a0864f29b6353caf0213501a",
   "display_name": "Python 3.7.2 64-bit ('d4ad_standardization': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}